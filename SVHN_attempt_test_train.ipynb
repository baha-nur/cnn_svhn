{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gen_input\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_test_valid_split = [1., 0., 0.]\n",
    "# From http://ufldl.stanford.edu/housenumbers/\n",
    "svhn_train = gen_input.read_data_sets(\"data/train_32x32.mat\", train_test_valid_split).train\n",
    "svhn_test = gen_input.read_data_sets(\"data/test_32x32.mat\", train_test_valid_split).train\n",
    "\n",
    "print svhn_train.images[0].shape\n",
    "print svhn_train.images.shape\n",
    "print svhn_test.images[0].shape\n",
    "print svhn_test.images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# per_img_means = svhn_train.images.mean(axis=1)\n",
    "# # per_img_norm = svhn_train.images - per_img_means[:, np.newaxis, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# for i in range(4010, 4025):\n",
    "#     fig = plt.figure()\n",
    "#     ax = fig.add_subplot(1,1,1)\n",
    "    \n",
    "#     img = svhn_train.images[i]\n",
    "#     lbl = np.argmax(svhn_train.labels[i])\n",
    "#     ax.set_title(lbl)\n",
    "#     img = img.reshape([32,32,3])\n",
    "#     imgplot = plt.imshow(img)\n",
    "#     plt.show()\n",
    "    \n",
    "# #     img_s = per_img_norm[i]\n",
    "# #     img_s = img_s.reshape([32,32,3])\n",
    "# #     imgplot = plt.imshow(img_s)\n",
    "# #     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data information\n",
    "n_input = 1024 # SVHN data input (img shape: 32*32)\n",
    "n_classes = 10 # total classes (0-9 digits)\n",
    "channels = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##########################################\n",
    "##                                      ##\n",
    "##              Parameters              ##\n",
    "##                                      ##\n",
    "##########################################\n",
    "# Training Parameters\n",
    "learning_rate = 1e-3\n",
    "training_epochs = 2 # <-- 2 for now\n",
    "batch_size = 100\n",
    "total_batches = int(0.025 * svhn_train.num_examples / batch_size) # <-- only 2.5% of data for now\n",
    "\n",
    "test_every = 5 # record test accuracy every 5 batches\n",
    "test_batch_size = int(0.025*svhn_test.num_examples) # <--- only 2.5% of the test batch for now\n",
    "final_test_size = test_batch_size*5\n",
    "\n",
    "# 1 - drop out \n",
    "train_keep_prob = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##########################################\n",
    "##                                      ##\n",
    "##            Helper Wrappers           ##\n",
    "##                                      ##\n",
    "##########################################\n",
    "# Conv2D wrapper, with bias and relu activation\n",
    "def conv2d(x, W, b, strides=1):\n",
    "    x = tf.nn.conv2d(x, W, \n",
    "                     strides=[1, strides, strides, 1], \n",
    "                     padding='SAME')\n",
    "    x = tf.nn.bias_add(x, b)\n",
    "    return tf.nn.relu(x)\n",
    "\n",
    "# MaxPool2D wrapper\n",
    "def maxpool2d(x, k=2):\n",
    "    return tf.nn.max_pool(x, \n",
    "                          ksize=[1, k, k, 1], \n",
    "                          strides=[1, k, k, 1],\n",
    "                          padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create model\n",
    "def conv_net(x, weights, biases, keep_prob):\n",
    "    # Reshape input picture\n",
    "    x = tf.reshape(x, shape=[-1, 32, 32, channels])\n",
    "\n",
    "    conv1 = conv2d(x, weights['wc1'], biases['bc1'])\n",
    "    conv1 = maxpool2d(conv1, k=2)\n",
    "    tf.summary.histogram(\"conv1\", conv1)\n",
    "\n",
    "    conv2 = conv2d(conv1, weights['wc2'], biases['bc2'])\n",
    "    conv2 = maxpool2d(conv2, k=2)\n",
    "    tf.summary.histogram(\"conv2\", conv2)\n",
    "\n",
    "    # Fully connected layer\n",
    "    # Reshape conv2 output to fit fully connected layer input\n",
    "    fc1 = tf.reshape(conv2, [-1, weights['wd1'].get_shape().as_list()[0]])\n",
    "    fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])\n",
    "    fc1 = tf.nn.relu(fc1)\n",
    "    fc1 = tf.nn.dropout(fc1, keep_prob)\n",
    "\n",
    "    # Output classes\n",
    "    out = tf.add(tf.matmul(fc1, weights['out']), biases['out'])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Normalize by subtracting per image, per channel means\n",
    "def normalize_batch(batch):\n",
    "  per_img_ch_means = batch.mean(axis=1)\n",
    "  return batch - per_img_ch_means[:, np.newaxis, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ts = datetime.now().strftime('%Y%m%d_%H%M')\n",
    "logs_path = \"logs/{}/\".format(ts)\n",
    "\n",
    "##########################################\n",
    "##                                      ##\n",
    "##            Variable Scopes           ##\n",
    "##            (for neatness)            ##\n",
    "##########################################\n",
    "\n",
    "# RESET TF GRAPH, just in case\n",
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.name_scope('input'):\n",
    "    x = tf.placeholder(tf.float32, shape=[None, n_input, channels], name=\"x_input\") \n",
    "    y = tf.placeholder(tf.float32, shape=[None, n_classes], name=\"y_actual\")\n",
    "\n",
    "with tf.name_scope('dropout'):\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "with tf.name_scope('weights'):\n",
    "    weights = {\n",
    "        # 5x5 conv, 1 input, 32 outputs\n",
    "        'wc1': tf.Variable(tf.random_normal([5, 5, channels, 32]), name=\"weights_conv1\"), # 32x32\n",
    "        # 5x5 conv, 32 inputs, 64 outputs\n",
    "        'wc2': tf.Variable(tf.random_normal([5, 5, 32, 64]), name=\"weights_conv2\"), # 16x16\n",
    "        # fully connected, 7*7*64 inputs, 1024 outputs\n",
    "        'wd1': tf.Variable(tf.random_normal([8*8*64, 4096]), name=\"weights_fc1\"), # 8x8\n",
    "        # 1024 inputs, 10 outputs (class prediction)\n",
    "        'out': tf.Variable(tf.random_normal([4096, n_classes]), name=\"weights_output\")\n",
    "    }\n",
    "\n",
    "with tf.name_scope('biases'):\n",
    "    biases = {\n",
    "        'bc1': tf.Variable(tf.random_normal([32]), name=\"bias_conv1\"),\n",
    "        'bc2': tf.Variable(tf.random_normal([64]), name=\"bias_conv2\"),\n",
    "        'bd1': tf.Variable(tf.random_normal([4096]), name=\"bias_fc1\"),\n",
    "        'out': tf.Variable(tf.random_normal([n_classes]), name=\"bias_output\")\n",
    "    }\n",
    "\n",
    "with tf.name_scope('Model'):\n",
    "    y_ = conv_net(x, weights, biases, keep_prob)\n",
    "    \n",
    "with tf.name_scope('Loss'):\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_, labels=y))\n",
    "\n",
    "with tf.name_scope('Accuracy'):\n",
    "    accuracy = tf.equal(tf.argmax(y_, 1), tf.argmax(y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(accuracy, tf.float32))\n",
    "    test_accuracy = tf.equal(tf.argmax(y_, 1), tf.argmax(y, 1))\n",
    "    test_accuracy = tf.reduce_mean(tf.cast(accuracy, tf.float32))\n",
    "\n",
    "with tf.name_scope('Optimizer'):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    \n",
    "    # Op to calculate every variable gradient\n",
    "    grads = tf.gradients(loss, tf.trainable_variables())\n",
    "    grads = list(zip(grads, tf.trainable_variables()))\n",
    "    \n",
    "    # Op to update all variables according to their gradient\n",
    "    apply_grads = optimizer.apply_gradients(grads_and_vars=grads)\n",
    "\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "\n",
    "##########################################\n",
    "##                                      ##\n",
    "##               Metrics                ##\n",
    "##           (for tensorboard)          ##\n",
    "##########################################\n",
    "# Summaries to visualize loss & accuracy\n",
    "tf.summary.scalar(\"loss\", loss)\n",
    "tf.summary.scalar(\"accuracy\", accuracy)\n",
    "\n",
    "# # Summaries to visualize weights\n",
    "# for var in tf.trainable_variables():\n",
    "#     var_name = var.name.replace(\":\", \"_\") # to suppress the pesky warning\n",
    "#     tf.summary.histogram(var_name + '_weights', var)\n",
    "    \n",
    "# # Summaries to visualize gradients\n",
    "# for grad, var in grads:\n",
    "#     var_name = var.name.replace(\":\", \"_\") # to suppress the pesky warning\n",
    "#     tf.summary.histogram(var_name + '_gradient', grad)\n",
    "\n",
    "merged_summaries = tf.summary.merge_all()\n",
    "\n",
    "tf.summary.scalar(\"test_accuracy\", test_accuracy)\n",
    "test_summaries = tf.summary.\n",
    "\n",
    "##########################################\n",
    "##                                      ##\n",
    "##           Launch the graph           ##\n",
    "##                                      ##\n",
    "##########################################\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    summary_writer = tf.summary.FileWriter(logs_path, graph=tf.get_default_graph())\n",
    "\n",
    "    # TRAINING MODEL\n",
    "    for epoch in xrange(training_epochs):\n",
    "        avg_loss = 0.0\n",
    "\n",
    "        for batch_num in range(total_batches):\n",
    "            batch_x, batch_y = svhn_train.next_batch(batch_size)\n",
    "            batch_x = normalize_batch(batch_x)\n",
    "            \n",
    "            _, batch_loss, batch_summary = sess.run([apply_grads, loss, merged_summaries],\n",
    "                                                    feed_dict={x: batch_x, y: batch_y,\n",
    "                                                               keep_prob: train_keep_prob})\n",
    "\n",
    "            summary_writer.add_summary(batch_summary, epoch * total_batches + batch_num)\n",
    "\n",
    "            avg_loss += batch_loss / total_batches\n",
    "            \n",
    "            # Every few batches, record test accuracy using same drop-out and epoch counter\n",
    "            if batch_num % test_every == 0: \n",
    "                test_batch_x, test_batch_y = svhn_test.next_batch(test_batch_size)                 \n",
    "                test_batch_x = normalize_batch(test_batch_x)\n",
    "\n",
    "                batch_test_accuracy = sess.run(test_accuracy, feed_dict={x: test_batch_x, y: test_batch_y,\n",
    "                                                     keep_prob: train_keep_prob})\n",
    "                \n",
    "                summary_writer.add_summary(batch_test_accuracy, epoch * total_batches + batch_num)\n",
    "            \n",
    "        # Display logs per epoch step\n",
    "        print \"Epoch:\", '%04d' % (epoch+1), \"cost =\", \"{:.9f}\".format(avg_loss)\n",
    "\n",
    "    print \"\\nOptimization Finished!\\n\"\n",
    "\n",
    "    # TESTING MODEL ACCURACY AGAINST TEST SET\n",
    "    test_batch_x, test_batch_y = svhn_test.next_batch(final_test_size)\n",
    "    test_batch_x = normalize_batch(test_batch_x)\n",
    "    print \"Accuracy:\", sess.run(test_accuracy, feed_dict={x: test_batch_x, y: test_batch_y,\n",
    "                                                     keep_prob: 1.0}) # No dropout\n",
    "    \n",
    "    print \"-\"* 70\n",
    "    pwd = os.getcwd()+\"/\"\n",
    "    print(\"Run the following to start tensorboard server:\\n\" \\\n",
    "          \"tensorboard --logdir=/{}{}\".format(pwd, logs_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
