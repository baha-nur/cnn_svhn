{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gen_input\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1024, 3)\n",
      "(73257, 1024, 3)\n",
      "(1024, 3)\n",
      "(26032, 1024, 3)\n"
     ]
    }
   ],
   "source": [
    "train_test_valid_split = [1., 0., 0.]\n",
    "# From http://ufldl.stanford.edu/housenumbers/\n",
    "svhn_train = gen_input.read_data_sets(\"data/train_32x32.mat\", train_test_valid_split).train\n",
    "svhn_test = gen_input.read_data_sets(\"data/test_32x32.mat\", train_test_valid_split).train\n",
    "\n",
    "print svhn_train.images[0].shape\n",
    "print svhn_train.images.shape\n",
    "print svhn_test.images[0].shape\n",
    "print svhn_test.images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# per_img_means = svhn_train.images.mean(axis=1)\n",
    "# # per_img_norm = svhn_train.images - per_img_means[:, np.newaxis, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# for i in range(4010, 4025):\n",
    "#     fig = plt.figure()\n",
    "#     ax = fig.add_subplot(1,1,1)\n",
    "    \n",
    "#     img = svhn_train.images[i]\n",
    "#     lbl = np.argmax(svhn_train.labels[i])\n",
    "#     ax.set_title(lbl)\n",
    "#     img = img.reshape([32,32,3])\n",
    "#     imgplot = plt.imshow(img)\n",
    "#     plt.show()\n",
    "    \n",
    "# #     img_s = per_img_norm[i]\n",
    "# #     img_s = img_s.reshape([32,32,3])\n",
    "# #     imgplot = plt.imshow(img_s)\n",
    "# #     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data information\n",
    "input_channels = 3\n",
    "image_size = 32\n",
    "n_classes = 10 # total classes (0-9 digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##########################################\n",
    "##                                      ##\n",
    "##              Parameters              ##\n",
    "##                                      ##\n",
    "##########################################\n",
    "# Training Parameters\n",
    "learning_rate = 1e-3\n",
    "training_epochs = 2 # <-- 2 for now\n",
    "batch_size = 100 #\n",
    "total_batches = int(0.025 * svhn_train.num_examples / batch_size) # <-- only 2.5% of data for now\n",
    "\n",
    "test_every = 5 # record test accuracy every 5 batches\n",
    "test_batch_size = int(0.025*svhn_test.num_examples) # <--- only 2.5% of the test batch for now\n",
    "final_test_size = test_batch_size*5\n",
    "\n",
    "# 1 - drop out \n",
    "train_keep_prob = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ts = datetime.now().strftime('%Y%m%d_%H%M')\n",
    "logs_path = \"logs/{}/\".format(ts)\n",
    "\n",
    "# RESET TF GRAPH, just in case\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##########################################\n",
    "##                                      ##\n",
    "##            Helper Wrappers           ##\n",
    "##                                      ##\n",
    "##########################################\n",
    "\n",
    "# We can't initialize these variables to 0 - the network will get stuck.\n",
    "def weight_variable(shape, stddev=0.1):\n",
    "  \"\"\"Create a weight variable with appropriate initialization.\"\"\"\n",
    "  initial = tf.truncated_normal(shape, stddev)\n",
    "  return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape, default_bias=0.1):\n",
    "  \"\"\"Create a bias variable with appropriate initialization.\"\"\"\n",
    "  initial = tf.constant(default_bias, shape=shape)\n",
    "  return tf.Variable(initial)\n",
    "\n",
    "def variable_summaries(var):\n",
    "  \"\"\"Attach a lot of summaries to a Tensor (for TensorBoard visualization).\"\"\"\n",
    "  with tf.name_scope('summaries'):\n",
    "    mean = tf.reduce_mean(var)\n",
    "    tf.summary.scalar('mean', mean)\n",
    "    with tf.name_scope('stddev'):\n",
    "      stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "    tf.summary.scalar('stddev', stddev)\n",
    "    tf.summary.scalar('max', tf.reduce_max(var))\n",
    "    tf.summary.scalar('min', tf.reduce_min(var))\n",
    "    tf.summary.histogram('histogram', var)\n",
    "\n",
    "def convlayer(layer_name, input_tensor, receptive_field, channels_in, channels_out,\n",
    "              padding='SAME', stride=1, act=tf.nn.relu, \n",
    "              pool=True, pooler=tf.nn.max_pool, pool_size=2, pool_stride=2, pool_padding='SAME'):\n",
    "  \"\"\"General purpose convolutional layer, followed by pooling\n",
    "\n",
    "  It does a matrix convolution, bias add, and then uses relu by default to nonlinearize.\n",
    "  Then it pools using max pooling by default.\n",
    "  It also sets up name scoping so that the resultant graph is easy to read,\n",
    "  and adds a number of summary ops for TensorBoard.\n",
    "  \"\"\"\n",
    "  # Adding a name scope ensures logical grouping of the layers in the graph.\n",
    "  with tf.name_scope(layer_name):\n",
    "    # This Variable will hold the state of the weights for the layer\n",
    "    with tf.name_scope('weights'):\n",
    "      weights = weight_variable([receptive_field, receptive_field, channels_in, channels_out])\n",
    "      variable_summaries(weights)\n",
    "    with tf.name_scope('biases'):\n",
    "      biases = bias_variable([channels_out])\n",
    "      variable_summaries(biases)\n",
    "    with tf.name_scope('W_conv_x_plus_b'):\n",
    "      preactivate = tf.nn.conv2d(input_tensor, weights, \n",
    "                                 strides=[1, stride, stride, 1], \n",
    "                                 padding=padding) + biases\n",
    "      tf.summary.histogram('pre_activations', preactivate)\n",
    "      \n",
    "    activations = act(preactivate, name='activation')\n",
    "    tf.summary.histogram('activations', activations)\n",
    "    \n",
    "    if pool:\n",
    "      max_pool = pooler(activations, ksize=[1, pool_size, pool_size, 1], \n",
    "                      strides=[1, pool_stride, pool_stride, 1],\n",
    "                      padding=pool_padding)\n",
    "\n",
    "      tf.summary.histogram('pools', max_pool)\n",
    "      return max_pool\n",
    "    else: \n",
    "      return activations\n",
    "    \n",
    "def nn_layer(layer_name, input_tensor, input_dim, output_dim, act=tf.nn.relu):\n",
    "  \"\"\"Reusable code for making a normal neural net layer.\n",
    "  It does a matrix multiply, bias add, and then uses ReLU to nonlinearize.\n",
    "  It also sets up name scoping so that the resultant graph is easy to read,\n",
    "  and adds a number of summary ops.\n",
    "  \"\"\"\n",
    "  # Adding a name scope ensures logical grouping of the layers in the graph.\n",
    "  with tf.name_scope(layer_name):\n",
    "    # This Variable will hold the state of the weights for the layer\n",
    "    with tf.name_scope('weights'):\n",
    "      weights = weight_variable([input_dim, output_dim])\n",
    "      variable_summaries(weights)\n",
    "    with tf.name_scope('biases'):\n",
    "      biases = bias_variable([output_dim])\n",
    "      variable_summaries(biases)\n",
    "    with tf.name_scope('Wx_plus_b'):\n",
    "      preactivate = tf.matmul(input_tensor, weights) + biases\n",
    "      tf.summary.histogram('pre_activations', preactivate)\n",
    "    activations = act(preactivate, name='activation')\n",
    "    tf.summary.histogram('activations', activations)\n",
    "    return activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Input placeholders\n",
    "with tf.name_scope('input'):\n",
    "  x = tf.placeholder(tf.float32, shape=[None, image_size*image_size, input_channels], name=\"x-input\") \n",
    "  y_ = tf.placeholder(tf.float32, shape=[None, n_classes], name=\"y-input\")\n",
    "\n",
    "with tf.name_scope('input_reshape'):\n",
    "  image_shaped_input = tf.reshape(x, [-1, image_size, image_size, input_channels])\n",
    "  tf.summary.image('input', image_shaped_input, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Architecture\n",
    "\n",
    "conv1 = convlayer(layer_name='conv1', input_tensor=image_shaped_input, receptive_field=5, \n",
    "                    channels_in=input_channels, channels_out=16)\n",
    "\n",
    "conv2 = convlayer(layer_name='conv2', input_tensor=conv1, receptive_field=5, \n",
    "                  channels_in=16, channels_out=32)\n",
    "\n",
    "dim = 1 # Compute how many numbers we have, ignoring the batch size\n",
    "for d in conv2.get_shape()[1:].as_list():\n",
    "  dim *= d\n",
    "\n",
    "with tf.name_scope('conv2_reshape'):\n",
    "  conv_shaped = tf.reshape(conv2, [-1, dim])\n",
    "\n",
    "fc1 = nn_layer(layer_name='fc1', input_tensor=conv_shaped, input_dim=dim, output_dim=64)\n",
    "\n",
    "with tf.name_scope('dropout'):\n",
    "  keep_prob = tf.placeholder(tf.float32)\n",
    "  tf.summary.scalar('dropout_keep_probability', keep_prob)\n",
    "  dropped = tf.nn.dropout(fc1, keep_prob)\n",
    "  \n",
    "# Do not apply softmax activation yet! use the identity\n",
    "y = nn_layer(layer_name='output', input_tensor=dropped, input_dim=64, output_dim=10, act=tf.identity) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('cross_entropy'):\n",
    "  # The raw formulation of cross-entropy,\n",
    "  #\n",
    "  # tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(tf.softmax(y)),\n",
    "  #                               reduction_indices=[1]))\n",
    "  #\n",
    "  # can be numerically unstable.\n",
    "  #\n",
    "  # So here we use tf.nn.softmax_cross_entropy_with_logits on the\n",
    "  # raw outputs of the nn_layer above, and then average across\n",
    "  # the batch.\n",
    "  diff = tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y)\n",
    "  with tf.name_scope('total'):\n",
    "    cross_entropy = tf.reduce_mean(diff)\n",
    "\n",
    "tf.summary.scalar('cross_entropy', cross_entropy)\n",
    "\n",
    "with tf.name_scope('train'):\n",
    "  train_step = tf.train.AdamOptimizer(learning_rate).minimize(\n",
    "      cross_entropy)\n",
    "\n",
    "with tf.name_scope('accuracy'):\n",
    "  with tf.name_scope('correct_prediction'):\n",
    "    correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "  with tf.name_scope('accuracy'):\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "tf.summary.scalar('accuracy', accuracy)\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "# Merge all the summaries and write them out\n",
    "merged = tf.summary.merge_all()\n",
    "train_writer = tf.summary.FileWriter(logs_path + '/train', sess.graph)\n",
    "test_writer = tf.summary.FileWriter(logs_path + '/test')\n",
    "\n",
    "tf.global_variables_initializer().run()\n",
    "\n",
    "\n",
    "# Normalize by subtracting per image, per channel means\n",
    "def normalize_batch(batch):\n",
    "  per_img_ch_means = batch.mean(axis=1)\n",
    "  return batch - per_img_ch_means[:, np.newaxis, :]\n",
    "\n",
    "# Train the model, and also write summaries.\n",
    "# Every 10th step, measure test-set accuracy, and write test summaries\n",
    "# All other steps, run train_step on training data, & add training summaries\n",
    "\n",
    "def feed_dict(train):\n",
    "  \"\"\"Make a TensorFlow feed_dict: maps data onto Tensor placeholders.\"\"\"\n",
    "  if train:\n",
    "    batch_x, batch_y = svhn_train.next_batch(batch_size)\n",
    "    batch_x = normalize_batch(batch_x)\n",
    "    keep_proba = train_keep_prob\n",
    "  else:\n",
    "    batch_x, batch_y = svhn_test.next_batch(test_batch_size)\n",
    "    keep_proba = 1.0\n",
    "  return {x: batch_x, y_: batch_y, keep_prob: keep_proba}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy at epoch 0 - step 5: 0.184615\n",
      "('Adding run metadata for', 9)\n",
      "Accuracy at epoch 0 - step 10: 0.149231\n",
      "Accuracy at epoch 0 - step 15: 0.173846\n",
      "Accuracy at epoch 1 - step 5: 0.161538\n",
      "('Adding run metadata for', 27)\n",
      "Accuracy at epoch 1 - step 10: 0.127692\n",
      "Accuracy at epoch 1 - step 15: 0.158462\n",
      "\n",
      "Optimization Finished!\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Run the following to start tensorboard server:\n",
      "tensorboard --logdir=//home/andy/Desktop/cnn_svhn/logs/20170421_2034/\n"
     ]
    }
   ],
   "source": [
    "for epoch in xrange(training_epochs):\n",
    "  for batch_num in xrange(total_batches):\n",
    "    if batch_num % test_every == 0 and batch_num > 0:  \n",
    "      # Record summaries and accuracy on the *test* set\n",
    "      summary, acc = sess.run([merged, accuracy], feed_dict=feed_dict(train=False))\n",
    "      test_writer.add_summary(summary, epoch * total_batches + batch_num)\n",
    "      print('Accuracy at epoch %s - step %s: %s' % (epoch, batch_num, acc))\n",
    "    else:  # Record train set summaries, and train\n",
    "      if batch_num% 10 == 9:  \n",
    "        # Record execution stats\n",
    "        run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n",
    "        run_metadata = tf.RunMetadata()\n",
    "        summary, _ = sess.run([merged, train_step],\n",
    "                              feed_dict=feed_dict(True),\n",
    "                              options=run_options,\n",
    "                              run_metadata=run_metadata)\n",
    "        train_writer.add_run_metadata(run_metadata, 'step{}'.format(epoch * total_batches + batch_num))\n",
    "        train_writer.add_summary(summary, epoch * total_batches + batch_num)\n",
    "        print('Adding run metadata for', epoch * total_batches + batch_num)\n",
    "      else:  \n",
    "        # Record a normal summary on training data\n",
    "        summary, _ = sess.run([merged, train_step], feed_dict=feed_dict(True))\n",
    "        train_writer.add_summary(summary, epoch * total_batches + batch_num)\n",
    "  \n",
    "print \"\\nOptimization Finished!\\n\"\n",
    "print \"-\"* 70\n",
    "pwd = os.getcwd()+\"/\"\n",
    "print(\"Run the following to start tensorboard server:\\n\" \\\n",
    "      \"tensorboard --logdir=/{}{}\".format(pwd, logs_path))\n",
    "\n",
    "train_writer.close()\n",
    "test_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# with tf.name_scope('Optimizer'):\n",
    "#     optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    \n",
    "#     # Op to calculate every variable gradient\n",
    "#     grads = tf.gradients(loss, tf.trainable_variables())\n",
    "#     grads = list(zip(grads, tf.trainable_variables()))\n",
    "    \n",
    "#     # Op to update all variables according to their gradient\n",
    "#     apply_grads = optimizer.apply_gradients(grads_and_vars=grads)\n",
    "\n",
    "# # Summaries to visualize gradients\n",
    "# for grad, var in grads:\n",
    "#     var_name = var.name.replace(\":\", \"_\") # to suppress the pesky warning\n",
    "#     tf.summary.histogram(var_name + '_gradient', grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
